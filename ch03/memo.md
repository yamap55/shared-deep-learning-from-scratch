# 3 章 ニューラルネットワーク

## 注意とか

- 図は ~~面倒なので~~ 割愛。図 3.1 とか書いて、本を見つつ説明することとします。
  - 式は書いています
- 説明者は普段ニューラルネットワークなど一切使用しておらず本当の初見であり、数学的知識もそれほどないです。
- 説明中にツッコミを歓迎します。随時止めてツッコミお願いします。
- わからなかった点、理解が浅くてよくわからなかった事が多数あるため、説明中に名指しで聞きますので何らかの反応くれると助かります。
  - 参加者？の上から聞く予定です。

## 3.1 パーセプトロンからニューラルネットワークへ

パーセプトロンとニューラルネットワークは似てる。異なる点を中心にニューラルネットワークの仕組みを解説していく。

### 3.1.1 ニューラルネットワークの例

- 左の列を入力層、右の列を出力層、中間の列を中間層（隠れ層）と呼ぶ
- 本書では入力層から出力層へ第 0 層、第 1 層、第 2 層と呼ぶこととする
- 図 3-1 を見ると、ニューロンのつながり方に関しては何ら変わりがない。

### 3.1.2 パーセプトロンの復習

- 図 3-2 は $x_1$, $x_2$ の 2 つの入力信号を受け取り $y$ を出力するパーセプトロン
- 図 3−2 のパーセプトロンを数式で表すと下記の式 3.1 で表される
- $b$ はバイアス : ニューロンの発火のしやすさをコントロール
- $w_1$, $w_2$ は各信号の重みを表す
  - 各信号の重要性をコントロール
- 図 3-2 にバイアス $b$ を追加したものが図 3-3
  - 重みが $b$ で入力が $1$ の信号を追加
- 式 3.1 をシンプルな形にするために式 3.2 式 3.3 に書き換える
  - $h(x)$ という関数を導入することで、シンプルになった。

#### 式 3.1

$$
y = \begin{cases}
0 & (b + w_1 x_1 + w_2 x_2 \leqq 0)\\
1 & (b + w_1 x_1 + w_2 x_2 > 0)
\end{cases}
$$

#### 式 3.2

$$
y = h(b + w_1 x_1 + w_2 x_2)
$$

#### 式 3.3

$$
h(x) = \begin{cases}
0 & (x \leqq 0)\\
1 & (x > 0)
\end{cases}
$$

### 3.1.3 活性化関数の登場

- $h(x)$ のような入力信号の総和を出力信号に変換する関数を一般に活性化関数と呼ぶ。
  - 活性化関数は入力信号の総和がどのように活性化（発火）するかを決定する役割
- 式 3.2 を丁寧に書くと式 3.4 式 3.5 の 2 つの式に分けられる
  - 式 3.3 は含まれない
- 式 3.4 式 3.5 を明示的に示すことで 図 3−4 のように表される

#### 式 3.4

$$
a = h(b + w_1 x_1 + w_2 x_2)
$$

#### 式 3.5

$$
y = h(a)
$$

## 3.2 活性化関数

- 式 3.3 で表される活性化関数は、閾値によって「出力が切り替わる」
  - このような関数をステップ関数や階段関数と呼ばれる
- パーセプトロンは活性化関数にステップ関数を利用している
- ステップ関数を別の関数に変更すると、ニューラルネットワークの世界に進むことができる

### 3.2.1 シグモイド関数

- ニューラルネットワークでよく用いられる活性化関数のひとつに式 3.6 で表されるシグモイド関数がある
- $exp(-x)$ は $e^{-x}$ を意味する
- $e$ はネイピア数の 2.7182... の実数を表す
  - > ネイピア数は数学定数の一つであり、自然対数の底である。
  - https://ja.wikipedia.org/wiki/%E3%83%8D%E3%82%A4%E3%83%94%E3%82%A2%E6%95%B0
  - 何言ってるのかさっぱりだったので誰か教えてください
- 複雑そうだが、ただの関数。入力によって出力が変わる
  - ここでは詳細よりもシグモイド関数を使うよ。位の認識で良いという事？

#### 式 3.6

$$
h(x) = \frac{1}{1 + exp(-x)}
$$

### 3.2.2 ステップ関数の実装

ch03.ipynb 参照

### 3.2.3 ステップ関数のグラフ

ch03.ipynb 参照

### 3.2.4 シグモイド関数の実装

ch03.ipynb 参照

### 3.2.5 シグモイド関数とステップ関数の比較

#### 異なる点

- ステップ関数とシグモイド関数を図 3-8 に示す
- 滑らかさが違う
  - この滑らかさがニューラルネットワークの学習で重要な意味を持つ
- 返り値が 0 or 1 と、0 から 1 までの実数を返すという違い

#### 共通点

- 大きな視点で見ると同じような形
- 返り値が 0 から 1 までの範囲におさまる

- 共通点については特にニューラルネットワーク云々という記載はなかったが、これは重要な点なの？

### 3.2.6 非線形関数

- ステップ関数とシグモイド関数の共通点は非線形関数という点もある
  - 非線形 -> 返り値が 1 本の直線ではない
- ニューラルネットワークでは活性化関数に線形関数を用いてはならない
- 線形の場合、1 回の掛け算で算出できてしまうため、隠れ層のないネットワークで実現できてしまうため
  - 要は意味がないという事で良い？

### 3.2.7 ReLU 関数

- 最近では ReLU 関数が主に用いられる
  - 図 3-9
- ch03.ipynb に実装を記載

#### 式 3.7

$$
h(x) = \begin{cases}
x & (x > 0)\\
0 & (x \leqq 0)
\end{cases}
$$

## 3.3 多次元配列の計算

Numpy の多次元配列についての説明

### 3.3.1 多次元配列

ch03.ipynb 参照

### 3.3.2 行列の積

ch03.ipynb 参照

### 3.3.3 ニューラルネットワークの行列の積

ch03.ipynb 参照
